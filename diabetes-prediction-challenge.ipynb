{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a792509a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T17:04:08.872634Z",
     "iopub.status.busy": "2025-12-31T17:04:08.872318Z",
     "iopub.status.idle": "2025-12-31T20:23:22.587839Z",
     "shell.execute_reply": "2025-12-31T20:23:22.586742Z"
    },
    "papermill": {
     "duration": 11953.721508,
     "end_time": "2025-12-31T20:23:22.589883",
     "exception": false,
     "start_time": "2025-12-31T17:04:08.868375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Fold 1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446674, number of negative: 273326\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3458\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620381 -> initscore=0.491164\n",
      "[LightGBM] [Info] Start training from score 0.491164\n",
      "ðŸ”¹ Fold 2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446674, number of negative: 273326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3456\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620381 -> initscore=0.491164\n",
      "[LightGBM] [Info] Start training from score 0.491164\n",
      "ðŸ”¹ Fold 3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446674, number of negative: 273326\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3462\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620381 -> initscore=0.491164\n",
      "[LightGBM] [Info] Start training from score 0.491164\n",
      "ðŸ”¹ Fold 4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446674, number of negative: 273326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3458\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620381 -> initscore=0.491164\n",
      "[LightGBM] [Info] Start training from score 0.491164\n",
      "ðŸ”¹ Fold 5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446674, number of negative: 273326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3450\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620381 -> initscore=0.491164\n",
      "[LightGBM] [Info] Start training from score 0.491164\n",
      "ðŸ”¹ Fold 6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446675, number of negative: 273325\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3457\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620382 -> initscore=0.491170\n",
      "[LightGBM] [Info] Start training from score 0.491170\n",
      "ðŸ”¹ Fold 7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446675, number of negative: 273325\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3447\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620382 -> initscore=0.491170\n",
      "[LightGBM] [Info] Start training from score 0.491170\n",
      "ðŸ”¹ Fold 8\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446675, number of negative: 273325\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3462\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620382 -> initscore=0.491170\n",
      "[LightGBM] [Info] Start training from score 0.491170\n",
      "ðŸ”¹ Fold 9\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446675, number of negative: 273325\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3458\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620382 -> initscore=0.491170\n",
      "[LightGBM] [Info] Start training from score 0.491170\n",
      "ðŸ”¹ Fold 10\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 446675, number of negative: 273325\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3455\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.620382 -> initscore=0.491170\n",
      "[LightGBM] [Info] Start training from score 0.491170\n",
      "\n",
      "ðŸ”¥ FINAL CV ROC-AUC: 0.7164\n",
      "\n",
      "âœ… submission.csv created\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. LIBRARIES\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. LOAD DATA\n",
    "# ==============================================================================\n",
    "INPUT_PATH = \"/kaggle/input/playground-series-s5e12/\"\n",
    "\n",
    "train_main = pd.read_csv(f\"{INPUT_PATH}train.csv\")\n",
    "test_df = pd.read_csv(f\"{INPUT_PATH}test.csv\")\n",
    "\n",
    "new_df = pd.read_csv(\"/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv\")\n",
    "\n",
    "new_df = new_df.drop([\n",
    "    \"glucose_fasting\",\n",
    "    \"glucose_postprandial\",\n",
    "    \"insulin_level\",\n",
    "    \"hba1c\",\n",
    "    \"diabetes_risk_score\",\n",
    "    \"diabetes_stage\"\n",
    "], axis=1)\n",
    "\n",
    "new_df[\"id\"] = -1\n",
    "\n",
    "train_df = pd.concat([train_main, new_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "target = \"diagnosed_diabetes\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"pulse_pressure\"] = df[\"systolic_bp\"] - df[\"diastolic_bp\"]\n",
    "    df[\"bp_ratio\"] = df[\"systolic_bp\"] / (df[\"diastolic_bp\"] + 1)\n",
    "    df[\"sedentary_ratio\"] = df[\"screen_time_hours_per_day\"] / (df[\"physical_activity_minutes_per_week\"] + 1)\n",
    "    df[\"chol_hdl_ratio\"] = df[\"cholesterol_total\"] / df[\"hdl_cholesterol\"]\n",
    "    df[\"lipid_risk\"] = (\n",
    "        df[\"cholesterol_total\"] +\n",
    "        df[\"ldl_cholesterol\"] +\n",
    "        df[\"triglycerides\"]\n",
    "    ) / df[\"hdl_cholesterol\"]\n",
    "    df[\"bmi_age\"] = df[\"bmi\"] * df[\"age\"]\n",
    "    df[\"waist_bmi\"] = df[\"waist_to_hip_ratio\"] * df[\"bmi\"]\n",
    "\n",
    "    history_cols = [\n",
    "        \"family_history_diabetes\",\n",
    "        \"hypertension_history\",\n",
    "        \"cardiovascular_history\"\n",
    "    ]\n",
    "    df[\"history_sum\"] = df[history_cols].sum(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = engineer_features(train_df)\n",
    "test_df = engineer_features(test_df)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. FEATURE PREPARATION\n",
    "# ==============================================================================\n",
    "drop_cols = [\"id\", target]\n",
    "cat_cols = train_df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "X_cat = train_df.drop(columns=drop_cols)\n",
    "y = train_df[target]\n",
    "X_test_cat = test_df.drop(columns=[\"id\"])\n",
    "\n",
    "# Ensure strings for CatBoost\n",
    "for c in cat_cols:\n",
    "    X_cat[c] = X_cat[c].astype(str)\n",
    "    X_test_cat[c] = X_test_cat[c].astype(str)\n",
    "\n",
    "cat_features_idx = [X_cat.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# One-hot encoding for LGB/XGB\n",
    "X_ohe = pd.get_dummies(X_cat, drop_first=True)\n",
    "X_test_ohe = pd.get_dummies(X_test_cat, drop_first=True)\n",
    "\n",
    "X_ohe, X_test_ohe = X_ohe.align(X_test_ohe, axis=1, fill_value=0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. MODELS\n",
    "# ==============================================================================\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=1500,\n",
    "    depth=7,\n",
    "    learning_rate=0.03,\n",
    "    l2_leaf_reg=6,\n",
    "    eval_metric=\"AUC\",\n",
    "    loss_function=\"Logloss\",\n",
    "    cat_features=cat_features_idx,\n",
    "    random_seed=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=1200,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. STRATIFIED K-FOLD CV\n",
    "# ==============================================================================\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "oof_cat = np.zeros(len(X_cat))\n",
    "oof_lgb = np.zeros(len(X_cat))\n",
    "oof_xgb = np.zeros(len(X_cat))\n",
    "\n",
    "test_cat = np.zeros(len(X_test_cat))\n",
    "test_lgb = np.zeros(len(X_test_cat))\n",
    "test_xgb = np.zeros(len(X_test_cat))\n",
    "\n",
    "for fold, (tr, val) in enumerate(skf.split(X_cat, y)):\n",
    "    print(f\"ðŸ”¹ Fold {fold+1}\")\n",
    "\n",
    "    # CatBoost\n",
    "    cat_model.fit(X_cat.iloc[tr], y.iloc[tr])\n",
    "    oof_cat[val] = cat_model.predict_proba(X_cat.iloc[val])[:, 1]\n",
    "    test_cat += cat_model.predict_proba(X_test_cat)[:, 1] / skf.n_splits\n",
    "\n",
    "    # LightGBM\n",
    "    lgb_model.fit(X_ohe.iloc[tr], y.iloc[tr])\n",
    "    oof_lgb[val] = lgb_model.predict_proba(X_ohe.iloc[val])[:, 1]\n",
    "    test_lgb += lgb_model.predict_proba(X_test_ohe)[:, 1] / skf.n_splits\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model.fit(X_ohe.iloc[tr], y.iloc[tr])\n",
    "    oof_xgb[val] = xgb_model.predict_proba(X_ohe.iloc[val])[:, 1]\n",
    "    test_xgb += xgb_model.predict_proba(X_test_ohe)[:, 1] / skf.n_splits\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. ENSEMBLE\n",
    "# ==============================================================================\n",
    "oof_ensemble = 0.5 * oof_cat + 0.3 * oof_lgb + 0.2 * oof_xgb\n",
    "cv_auc = roc_auc_score(y, oof_ensemble)\n",
    "print(f\"\\nðŸ”¥ FINAL CV ROC-AUC: {cv_auc:.4f}\")\n",
    "\n",
    "final_test_pred = 0.5 * test_cat + 0.3 * test_lgb + 0.2 * test_xgb\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"diagnosed_diabetes\": final_test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nâœ… submission.csv created\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    },
    {
     "datasetId": 8316713,
     "sourceId": 13128284,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11960.983575,
   "end_time": "2025-12-31T20:23:24.220838",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-31T17:04:03.237263",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
